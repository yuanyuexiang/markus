#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆGNNè®­ç»ƒè„šæœ¬ - å¸¦è¯¦ç»†è¿›åº¦æ—¥å¿—
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv, global_mean_pool
import json
import numpy as np
from pathlib import Path
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from datetime import datetime
import time


class SignatureGNN(nn.Module):
    """ç­¾åéªŒè¯å›¾ç¥ç»ç½‘ç»œ"""
    
    def __init__(self, input_dim=6, hidden_dim=64, output_dim=128):
        super(SignatureGNN, self).__init__()
        self.conv1 = GCNConv(input_dim, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)
        self.conv3 = GCNConv(hidden_dim, output_dim)
        
    def forward(self, x, edge_index, batch):
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=0.2, training=self.training)
        
        x = self.conv2(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=0.2, training=self.training)
        
        x = self.conv3(x, edge_index)
        x = global_mean_pool(x, batch)
        
        return x


def load_keypoint_data(json_path):
    """åŠ è½½å…³é”®ç‚¹JSONæ•°æ®"""
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data


def extract_node_features(keypoints, width, height):
    """æå–èŠ‚ç‚¹ç‰¹å¾"""
    type_to_idx = {'endpoint': 0, 'junction': 1, 'corner': 2, 'bifurcation': 3}
    
    features = []
    for kp in keypoints:
        x_norm = kp['x'] / width
        y_norm = kp['y'] / height
        
        type_onehot = [0, 0, 0, 0]
        type_idx = type_to_idx.get(kp['type'], 0)
        type_onehot[type_idx] = 1
        
        feat = [x_norm, y_norm] + type_onehot
        features.append(feat)
    
    return torch.tensor(features, dtype=torch.float)


def compute_graph_edges(keypoints, max_distance=50):
    """è®¡ç®—å›¾çš„è¾¹"""
    edges = []
    n = len(keypoints)
    
    for i in range(n):
        for j in range(i+1, n):
            x1, y1 = keypoints[i]['x'], keypoints[i]['y']
            x2, y2 = keypoints[j]['x'], keypoints[j]['y']
            
            dist = np.sqrt((x1-x2)**2 + (y1-y2)**2)
            
            if dist <= max_distance:
                edges.append([i, j])
                edges.append([j, i])
    
    if not edges:
        edges = [[i, i] for i in range(n)]
    
    return torch.tensor(edges, dtype=torch.long).t().contiguous()


def create_graph_from_json(json_path):
    """ä»JSONæ–‡ä»¶åˆ›å»ºPyGå›¾å¯¹è±¡"""
    data_dict = load_keypoint_data(json_path)
    
    keypoints = data_dict['keypoints']
    width = data_dict['image_size']['width']
    height = data_dict['image_size']['height']
    
    x = extract_node_features(keypoints, width, height)
    edge_index = compute_graph_edges(keypoints, max_distance=50)
    
    return Data(x=x, edge_index=edge_index)


def prepare_dataset():
    """å‡†å¤‡è®­ç»ƒæ•°æ®é›† - å¸¦è¯¦ç»†æ—¥å¿—"""
    print("=" * 70)
    print("ğŸ“Š æ­¥éª¤1: å‡†å¤‡è®­ç»ƒæ•°æ®é›†")
    print("=" * 70)
    
    print("\n[1/5] æ‰«æJSONæ–‡ä»¶...")
    template_files = sorted(Path(".").glob("keypoints_template_*_auto.json"))
    query_files = sorted(Path(".").glob("keypoints_query_*_auto.json"))
    
    print(f"  âœ“ æ‰¾åˆ°æ¨¡æ¿ç­¾å: {len(template_files)} ä¸ª")
    print(f"  âœ“ æ‰¾åˆ°æŸ¥è¯¢ç­¾å: {len(query_files)} ä¸ª")
    
    if len(template_files) == 0 or len(query_files) == 0:
        print("\nâŒ é”™è¯¯: æ²¡æœ‰æ‰¾åˆ°è¶³å¤Ÿçš„æ ‡æ³¨æ•°æ®!")
        return [], []
    
    # æå–æ—¶é—´æˆ³
    print("\n[2/5] æå–æ—¶é—´æˆ³...")
    def get_timestamp(filepath):
        name = filepath.stem
        parts = name.split('_')
        for i, part in enumerate(parts):
            if len(part) == 8 and part.isdigit():
                if i+1 < len(parts) and len(parts[i+1]) == 6 and parts[i+1].isdigit():
                    return f"{part}_{parts[i+1]}"
        return None
    
    template_by_time = {}
    for tf in template_files:
        ts = get_timestamp(tf)
        if ts:
            template_by_time[ts] = tf
    
    query_by_time = {}
    for qf in query_files:
        ts = get_timestamp(qf)
        if ts:
            query_by_time[ts] = qf
    
    print(f"  âœ“ è¯†åˆ«å‡º {len(template_by_time)} ä¸ªæ—¶é—´æˆ³")
    
    # åˆ›å»ºè®­ç»ƒå¯¹
    print("\n[3/5] åˆ›å»ºçœŸç­¾åå¯¹(Genuine pairs)...")
    pairs = []
    labels = []
    
    genuine_count = 0
    for ts in template_by_time.keys():
        if ts in query_by_time:
            pairs.append((template_by_time[ts], query_by_time[ts]))
            labels.append(1)
            genuine_count += 1
            print(f"  + Genuine pair {genuine_count}: {ts}")
    
    print(f"  âœ“ ç”Ÿæˆ {genuine_count} ä¸ªçœŸç­¾åå¯¹")
    
    print("\n[4/5] åˆ›å»ºå‡ç­¾åå¯¹(Forged pairs)...")
    forged_count = 0
    timestamps = list(template_by_time.keys())
    for i, ts1 in enumerate(timestamps):
        for ts2 in timestamps[i+1:]:
            if forged_count < genuine_count:
                pairs.append((template_by_time[ts1], query_by_time[ts2]))
                labels.append(0)
                forged_count += 1
                print(f"  + Forged pair {forged_count}: {ts1} vs {ts2}")
    
    print(f"  âœ“ ç”Ÿæˆ {forged_count} ä¸ªå‡ç­¾åå¯¹")
    
    print(f"\n[5/5] æ•°æ®é›†ç»Ÿè®¡:")
    print(f"  - çœŸç­¾åå¯¹: {genuine_count} ({genuine_count/len(pairs)*100:.1f}%)")
    print(f"  - å‡ç­¾åå¯¹: {forged_count} ({forged_count/len(pairs)*100:.1f}%)")
    print(f"  - æ€»è®¡: {len(pairs)} å¯¹")
    
    return pairs, labels


def preload_all_graphs(pairs):
    """é¢„åŠ è½½æ‰€æœ‰å›¾ä»¥åŠ é€Ÿè®­ç»ƒ - å¸¦è¿›åº¦æ¡"""
    print("\n" + "=" * 70)
    print("ğŸ“¦ æ­¥éª¤2: é¢„åŠ è½½æ‰€æœ‰å›¾æ•°æ®(é¿å…è®­ç»ƒæ—¶é‡å¤åŠ è½½)")
    print("=" * 70)
    
    unique_files = set()
    for template_path, query_path in pairs:
        unique_files.add(template_path)
        unique_files.add(query_path)
    
    unique_files = sorted(unique_files)
    print(f"\néœ€è¦åŠ è½½ {len(unique_files)} ä¸ªå”¯ä¸€å›¾...")
    
    graph_cache = {}
    start_time = time.time()
    
    for i, filepath in enumerate(unique_files, 1):
        graph = create_graph_from_json(filepath)
        graph_cache[filepath] = graph
        
        # æ¯10ä¸ªå›¾æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
        if i % 5 == 0 or i == len(unique_files):
            elapsed = time.time() - start_time
            rate = i / elapsed if elapsed > 0 else 0
            eta = (len(unique_files) - i) / rate if rate > 0 else 0
            print(f"  [{i:2d}/{len(unique_files)}] "
                  f"åŠ è½½: {filepath.name[:40]:<40} "
                  f"({graph.x.size(0):4d}èŠ‚ç‚¹, {graph.edge_index.size(1):5d}è¾¹) "
                  f"[{rate:.1f}å›¾/ç§’, ETA:{eta:.1f}ç§’]")
    
    elapsed = time.time() - start_time
    print(f"\nâœ“ é¢„åŠ è½½å®Œæˆ! è€—æ—¶: {elapsed:.2f}ç§’")
    
    return graph_cache


def train_siamese_gnn(pairs, labels, graph_cache, epochs=20):
    """è®­ç»ƒSiamese GNN - å¸¦è¯¦ç»†è¿›åº¦"""
    print("\n" + "=" * 70)
    print("ğŸš€ æ­¥éª¤3: å¼€å§‹è®­ç»ƒSiamese GNN")
    print("=" * 70)
    
    # åˆ’åˆ†æ•°æ®
    print("\n[1/3] åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
    train_pairs, test_pairs, train_labels, test_labels = train_test_split(
        pairs, labels, test_size=0.2, random_state=42, stratify=labels
    )
    
    print(f"  âœ“ è®­ç»ƒé›†: {len(train_pairs)} å¯¹")
    print(f"  âœ“ æµ‹è¯•é›†: {len(test_pairs)} å¯¹")
    
    # åˆ›å»ºæ¨¡å‹
    print("\n[2/3] åˆ›å»ºæ¨¡å‹...")
    model = SignatureGNN(input_dim=6, hidden_dim=64, output_dim=128)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    print(f"  âœ“ æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
    
    # è®­ç»ƒ
    print("\n[3/3] å¼€å§‹è®­ç»ƒå¾ªç¯...")
    print("-" * 70)
    print(f"{'Epoch':>6} | {'Loss':>8} | {'Train Acc':>10} | {'Test Acc':>9} | {'Time':>8}")
    print("-" * 70)
    
    train_losses = []
    train_accs = []
    test_accs = []
    
    for epoch in range(epochs):
        epoch_start = time.time()
        model.train()
        total_loss = 0
        train_correct = 0
        
        # è®­ç»ƒ
        for idx, ((template_path, query_path), label) in enumerate(zip(train_pairs, train_labels)):
            optimizer.zero_grad()
            
            # ä»ç¼“å­˜è·å–å›¾
            graph1 = graph_cache[template_path]
            graph2 = graph_cache[query_path]
            
            batch1 = torch.zeros(graph1.x.size(0), dtype=torch.long)
            batch2 = torch.zeros(graph2.x.size(0), dtype=torch.long)
            
            emb1 = model(graph1.x, graph1.edge_index, batch1)
            emb2 = model(graph2.x, graph2.edge_index, batch2)
            
            distance = F.pairwise_distance(emb1, emb2)
            
            label_tensor = torch.tensor([label], dtype=torch.float)
            margin = 1.0
            
            if label == 1:
                loss = distance ** 2
            else:
                loss = torch.clamp(margin - distance, min=0.0) ** 2
            
            loss = loss.mean()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            # è®¡ç®—è®­ç»ƒå‡†ç¡®ç‡
            threshold = 0.5
            pred = 1 if distance.item() < threshold else 0
            if pred == label:
                train_correct += 1
        
        avg_loss = total_loss / len(train_pairs)
        train_acc = train_correct / len(train_pairs)
        train_losses.append(avg_loss)
        train_accs.append(train_acc)
        
        # æµ‹è¯•
        model.eval()
        test_correct = 0
        with torch.no_grad():
            for (template_path, query_path), label in zip(test_pairs, test_labels):
                graph1 = graph_cache[template_path]
                graph2 = graph_cache[query_path]
                
                batch1 = torch.zeros(graph1.x.size(0), dtype=torch.long)
                batch2 = torch.zeros(graph2.x.size(0), dtype=torch.long)
                
                emb1 = model(graph1.x, graph1.edge_index, batch1)
                emb2 = model(graph2.x, graph2.edge_index, batch2)
                
                distance = F.pairwise_distance(emb1, emb2).item()
                threshold = 0.5
                pred = 1 if distance < threshold else 0
                
                if pred == label:
                    test_correct += 1
        
        test_acc = test_correct / len(test_pairs)
        test_accs.append(test_acc)
        
        epoch_time = time.time() - epoch_start
        
        print(f"{epoch+1:6d} | {avg_loss:8.4f} | {train_acc:9.2%} | {test_acc:8.2%} | {epoch_time:7.2f}s")
    
    print("-" * 70)
    print(f"âœ… è®­ç»ƒå®Œæˆ!")
    print(f"   æœ€ä½³æµ‹è¯•å‡†ç¡®ç‡: {max(test_accs):.2%}")
    print(f"   æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_accs[-1]:.2%}")
    
    # ä¿å­˜
    torch.save(model.state_dict(), 'signature_gnn_model.pth')
    print(f"   æ¨¡å‹å·²ä¿å­˜: signature_gnn_model.pth")
    
    # ç»˜å›¾
    plot_training_curves(train_losses, train_accs, test_accs)
    
    return model, test_accs[-1]


def plot_training_curves(train_losses, train_accs, test_accs):
    """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))
    
    epochs = list(range(1, len(train_losses) + 1))
    
    # Lossæ›²çº¿
    ax1.plot(epochs, train_losses, 'b-', linewidth=2, label='Train Loss')
    ax1.set_xlabel('Epoch', fontsize=12)
    ax1.set_ylabel('Loss', fontsize=12)
    ax1.set_title('Training Loss', fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    ax1.legend()
    
    # Accuracyæ›²çº¿
    ax2.plot(epochs, [acc * 100 for acc in train_accs], 'b-', linewidth=2, label='Train Acc')
    ax2.plot(epochs, [acc * 100 for acc in test_accs], 'g-', linewidth=2, label='Test Acc')
    ax2.set_xlabel('Epoch', fontsize=12)
    ax2.set_ylabel('Accuracy (%)', fontsize=12)
    ax2.set_title('Accuracy Curves', fontsize=14, fontweight='bold')
    ax2.grid(True, alpha=0.3)
    ax2.legend()
    
    plt.tight_layout()
    plt.savefig('training_curves.png', dpi=150)
    print(f"   è®­ç»ƒæ›²çº¿å·²ä¿å­˜: training_curves.png")
    plt.close()


if __name__ == '__main__':
    start_time = time.time()
    
    print("\n" + "=" * 70)
    print("ğŸ“ ç­¾åéªŒè¯GNNè®­ç»ƒç³»ç»Ÿ (ä¼˜åŒ–ç‰ˆ)")
    print("=" * 70)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # å‡†å¤‡æ•°æ®
    pairs, labels = prepare_dataset()
    
    if len(pairs) == 0:
        print("\nâŒ æ²¡æœ‰è¶³å¤Ÿçš„è®­ç»ƒæ•°æ®!")
        exit(1)
    
    # é¢„åŠ è½½å›¾
    graph_cache = preload_all_graphs(pairs)
    
    # è®­ç»ƒ
    model, final_acc = train_siamese_gnn(pairs, labels, graph_cache, epochs=20)
    
    total_time = time.time() - start_time
    
    print("\n" + "=" * 70)
    print("ğŸ‰ å…¨éƒ¨å®Œæˆ!")
    print("=" * 70)
    print(f"æ€»è€—æ—¶: {total_time:.2f}ç§’ ({total_time/60:.1f}åˆ†é’Ÿ)")
    print(f"æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {final_acc:.2%}")
    print(f"\nç”Ÿæˆæ–‡ä»¶:")
    print(f"  - signature_gnn_model.pth")
    print(f"  - training_curves.png")
    print(f"\nä¸‹ä¸€æ­¥:")
    if final_acc > 0.85:
        print(f"  âœ… å‡†ç¡®ç‡>{85}%, å¯ä»¥è€ƒè™‘éƒ¨ç½²")
    else:
        print(f"  âš ï¸  å‡†ç¡®ç‡<{85}%, å»ºè®®:")
        print(f"     1. å¢åŠ è®­ç»ƒæ•°æ®(æ›´å¤šç­¾åæ ·æœ¬)")
        print(f"     2. è°ƒæ•´è¶…å‚æ•°(hidden_dim, epochs)")
        print(f"     3. å°è¯•å…¶ä»–GNNæ¶æ„(GAT, GraphSAGE)")
    print("=" * 70 + "\n")
